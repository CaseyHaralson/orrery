## 7. Skills Synchronization Strategy

To maintain a single source of truth for skills while making them available to each tool’s expected location, we implement a synchronization mechanism. The goals of the sync system are:

  - Minimize duplication (we don’t want to manually edit the same instruction in 5 places).

  - Prevent divergence (tool-specific copies should always match canonical unless a deliberate compatibility fork is needed).

  - Keep things simple for developers (ideally editing the canonical agent/skills/ and running a command or CI job updates everything).

Possible approaches:

  1. Symbolic Links: On systems that support symlinks, we can symlink .claude/skills -> ../agent/skills, and similarly for .gemini/skills, etc. This way there is literally one set of files. This is the simplest if it works across dev environments.

      - Tradeoff: Symlinks in Git on Windows can be problematic (need specific git settings or get converted to text). On Windows without proper permissions, symlinks might not work at all unless developer enables them. Also, some tools might not follow symlinks due to sandboxing.

  2. Manual Copy on Change: Use a small script or Makefile target to copy agent/skills/* into each of the .tool/skills dirs. This could be run whenever skills change (perhaps as a git pre-commit hook or just manually). We could also have CI enforce that no discrepancy exists.

      - Tradeoff: Developer might forget to run the sync script. But we can mitigate with a pre-commit hook or CI check.

  3. Git submodule or subtree: Possibly treat agent/skills as its own submodule and include it in each tool dir. This seems overkill and complicates development flow.

  4. Tool Config pointing to canonical skills: If any tool allowed configuring the skills directory path (e.g., a setting like skills.directory = agent/skills), that’d be ideal. Currently, not aware of such option in these CLIs, except Gemini’s context.fileName setting which is more for context files, not skill dirs.

  5. Use an external library like SkillPort which loads the skills from one location for all. But that’s more at runtime than in file system.

Our plan: Use copying with CI enforcement for broad compatibility. Symlinks where possible for convenience.

### Development Workflow for Skills:

  - Developers edit agent/skills/<skill>/SKILL.md. They can test changes using one of the agents (e.g., run a Claude session and manually load the skill, or in a dry-run mode of our coordinator script).

  - After editing, run tools/sync_skills.py (for example) which will:

    - Remove existing .tool/skills/<skill> dirs (or just overwrite specific files).

    - Copy the entire agent/skills/<skill> folder into each .tool/skills/ location.

    - Maybe adjust minor things if needed per tool (for instance, if we needed to strip out a part of a skill for a certain tool due to limitations, the sync script could apply a filter based on frontmatter compatibility flags).

  - The script could also validate the skills via the skills-ref library’s validator to catch format issues.

  - Then developers commit changes including the updated copies (so in Git, everything is consistent).

We will add a check in CI that runs on pull requests: It re-runs the sync script and then checks git status to see if any changes would occur (meaning someone forgot to sync). If yes, the CI fails telling them to sync. This ensures no one merges an update to agent/skills without also updating the tool dirs.

Alternatively, we could .gitignore the tool-specific skill copies (so that only agent/skills is versioned). Then have a post-checkout hook or initial setup script to populate them. But that complicates usage for others pulling the repo. It’s usually better to keep them in version control for transparency (others can see exactly what instructions each tool is getting and debug if needed).

### Transformations for Tool Differences:

In general, we want identical content. But if we had to tailor, how?

  - The Agent Skills spec provides compatibility field that could be used by tools to decide if they should load a skill. E.g., a skill might say compatibility: "Designed for Claude Code". If we wrote such, perhaps other agents would ignore it. But ideally our skills are universal and we use compatibility only to add notes.

  - If a tool needed a smaller context, we might shorten examples when syncing to that tool. For example, say OpenAI GPT-3.5 can’t handle a 300-line skill well; we could in sync script strip long examples from the GPT version (maybe maintain an alternate SKILL_gpt.md template). We prefer not to fork though.

  - Another case: a skill uses a Bash script. Cursor might run Node (bun) for hooks, not Bash. But Cursor does support Bash commands too. If not, we could provide a JS equivalent script in assets for Cursor, or instruct it accordingly.

At this time, we don’t foresee major tool-specific rewrites of skills. The standard’s whole point is one skill format works across agents. So likely minimal differences.

### OS Compatibility:
Our team might use different OS (Windows, Mac, Linux). The repository structure and sync should accommodate all:

  - If using symlinks on Windows, they must be created in a way that works (developer might need admin or Developer Mode). We will document how to enable symlinks on Windows or suggest using the copy method there.

  - The skills content themselves are OS-neutral (except any scripts: we might include both .sh and .ps1 if needed for any script in scripts/ directory, or just use Python which is cross-platform).

  - CI (which often runs Linux) will test the sync and possibly run some agent tasks in a headless way to ensure nothing OS-specific is broken.

### Automation in CI:
We can integrate a job that runs a minimal scenario for each agent to ensure the sync and integration:

  - For Claude/Gemini/Cursor: perhaps run a “plan only” command to see if skills are loaded (though those are interactive CLIs mostly – might be tricky to automate unless they have hidden API modes).

  - More feasible: run our orchestrator (likely using OpenAI or local mode) on a sample task and verify the output matches expectations as a regression test (see next section about evaluation).

  - Also, use the skills-ref validate command in CI to ensure all skills conform to spec.

### Optional Transformation Layer:
If needed, our sync could apply filters. For example:

  - Remove any comments in SKILL.md that a certain agent might erroneously interpret (not likely, but just in case).

  - Convert any non-UTF8 characters if a tool has an issue (all are modern so should be fine).

  - Possibly compile an “available_skills.xml” snippet for tools that need to explicitly add it. Actually, we might generate that on the fly in prompts rather than store a static one. But we could place an available_skills.xml file in .claude/ for example, and instruct users to do /append-system-prompt available_skills.xml if needed. This might be too manual; better to let code handle it.

Given the complexity of supporting many tools, we might also create a universal launcher script in Python that can interface with each:

  - e.g. run_with_agent.py --agent=claude --prompt "XYZ" which ensures the skills are loaded and the workflow followed. For Claude/Gemini, it might simply spawn the CLI process with appropriate flags (like claude --append-system-prompt .claude/available_skills.xml – if such flag exists). For OpenAI, it would call the API. This script isn’t strictly sync, but part of making usage easier.

In summary, our strategy:

  - Use copying for broad reliability (with optional symlinks for advanced users).

  - Automate with scripts and CI to avoid human error.

  - Provide documentation and perhaps a one-step command to “sync and validate skills”.

For example, make sync-skills could run the sync and make check-skills could run validations. If any differences or issues are found, developers fix them before committing.

Finally, we ensure bidirectional awareness: If a tool-specific change happens (say, an emergency quick fix is made in .claude/skills/plan/SKILL.md during a troubleshooting session), developers must propagate it back to canonical. To avoid forgetting, we might discourage editing the copies directly. Possibly even add a note at top of each copied file: “(autogenerated from agent/skills/plan/SKILL.md – do not edit here)”. This can be inserted by the sync script as an HTML comment or so if the agent doesn’t read it. But since agents do read SKILL.md fully on activation, we wouldn’t want that note visible to them. Maybe a one-line HTML comment is fine; LLM would likely ignore it or at least it wouldn’t harm. Alternatively, trust process and code review to catch such mishaps.

By keeping skills synchronized, we uphold the core promise: each agent is drawing from the same playbook. Now, we turn to how we will test and evaluate that consistency to ensure our system works as intended.