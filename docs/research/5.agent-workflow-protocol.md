## 5. Agent Workflow Protocol (Tool-Agnostic)

All agents, regardless of platform, will follow a unified 5-phase workflow for any significant coding task. This workflow is designed to impose structure and checkpoints, ensuring thoroughness and consistency. The phases are:

  1. Intake – Understand and clarify the task.

  2. Plan – Devise an execution plan or solution outline.

  3. Execute – Carry out the plan (write code or perform changes).

  4. Verify – Test and validate the work.

  5. Report – Summarize results and next steps.

Each phase produces defined outputs or artifacts and has clear expectations:

### Phase 1: Intake

Goal: Ensure the agent fully understands the request before proceeding. If requirements are ambiguous, ask clarifying questions; gather any relevant context from the codebase or documentation.

Agent Actions:

  - Summarize the user’s request in the agent’s own words.

  - If missing details (e.g., “What format should the CSV stats be in?”), ask the user for clarification (or refer to existing project docs if available).

  - Check project context: The agent might search the repository for related files (for instance, find if there's an existing CSV parser, or a similar feature to mimic) – tools like Cursor’s search or Claude’s knowledge integration could be used.

  - Confirm the final understood requirements by perhaps writing them to a intake.md or just storing in memory.

Output:

  - A refined task description or problem statement (could be stored in a file or displayed to user for confirmation).

  - Optionally, an “Intake log” noting Q&A with the user or any assumptions made.

Example: For the CSV upload feature, the intake phase might output: “Confirmed: We need to allow users to upload a CSV file via the UI, then the system should calculate basic stats (count, mean, median) and display them. We assume CSV size is moderate (<5MB) and use existing libs for CSV parsing.” This could be saved or just used as internal confirmation.

### Phase 2: Plan

Goal: Break the task into manageable steps (as discussed in section 4). Ensure the plan covers all aspects of the task and sequences them logically.

Agent Actions:

  - Using the plan skill, create a structured plan (likely the YAML as described).

  - Determine if multiple agents are needed (by assigning owners in the plan). Even if not actually launching separate agents, thinking in terms of roles (DB, front-end, etc.) ensures all areas are addressed.

  - Identify any potential parallel work or critical path.

  - Output the plan in a shareable format (file or directly in chat). Possibly also present a summary to the user for approval if interactive.

Required Output:

  - plan.yaml file with steps. This is the primary artifact of the Plan phase.

  - If the agent is interactive (like in a CLI where the developer is watching), it might also show the plan in a neat list format for the user to approve or refine. The plan should be saved to allow resumption (if process is interrupted, any agent can reload the plan and know what’s done/to-do).

Example: The agent produces the 4-step plan from earlier and perhaps prints:

  - “1 DB-Agent: Update DB schema for CSV”

  - “2️ Backend-Agent: Add /upload endpoint”

  - … etc., in a user-friendly way, while the raw YAML is stored.

### Phase 3: Execute

Goal: Implement the steps of the plan – i.e., write the code, configure systems, or perform the changes required.

Agent Actions:

  - Iterate through plan steps (or pick up assigned steps if multiple agents). For each:

    - Mark it as in-progress.

    - Open the relevant files or create new ones.

    - Write code or make changes according to the description, using best practices and the coding style guidelines (possibly referencing rules or context like CLAUDE.md or Cursor rules always loaded).

    - Use tools as needed: e.g., run a local development server or use an editor’s refactor commands if available. Many of our agents can issue shell commands to compile, run tests, etc. These should be done in this phase for immediate feedback (with permission checks).

    - If a subtask is complex, the agent could even generate a sub-plan internally, but ideally the plan was granular enough.

  - If an error or obstacle occurs (e.g., test fails, or code design needs revision), the agent might loop: fix code, re-run tests, etc., until success criteria is met or it determines it can’t resolve without new inputs. This is where having the verify step separate is useful: the agent might do basic verification within Execute, but anything not resolved will be caught in Verify explicitly.

  - Update status of each step upon completion (and potentially produce a mini-report for that step, as discussed).

Artifacts:

  - Source code files modified or created. These are the primary output.

  - Possibly commit(s) if using version control. Some agents like Claude Code and Gemini can auto-commit changes with messages(if allowed). We may configure them to commit at logical points (maybe after each major step or after all).

  - Execution logs or results for each step (e.g., output from a command).

Resuming Interrupted Execution: Because all changes are made to the repository (or a working copy), if an agent stops mid-way (say after completing step 2 of 4 due to a crash or user pause), another agent can resume by reading the updated plan (steps 1-2 marked complete, step 3 pending) and continuing. Additionally, partial code already written is in the repo, so the new agent can see it. This is an advantage of our file-based artifact approach – state is not just in the agent’s hidden memory but in the visible project state.

Error Handling & Rollback:

  - If something goes wrong during Execute (like the agent mis-edited a critical config and tests fail spectacularly), how do we handle it? We plan for agents to be cautious:

    - Use version control branches or checkpoints: e.g., before a major change, the agent might operate in a new git branch or commit frequently so changes can be reverted if needed.

    - The verify phase (or tests run) will catch issues; the agent can then either rollback (git revert) and try a different approach, or mark the plan step as blocked and escalate to human or a different strategy.

  - We will define in the WORKFLOW.md policy that agents should not permanently delete code without confirmation and should prefer additive changes. For rollback strategy, an automated approach is possible: if verify fails and the agent is stuck, it could reset to last commit and mark the step as needing human attention.

### Phase 4: Verify

Goal: Validate that the executed changes meet the acceptance criteria and that nothing is broken. This ensures quality and correctness before reporting completion.

Agent Actions:

  - Run test suites and linters. Agents have tools to execute tests (Claude can run commands, Gemini CLI has sandbox exec, Cursor can run code). For instance, run npm test or domain-specific tests.

  - Perform static analysis or verification scripts (security checks, performance checks if applicable). This could leverage a verify skill that includes steps like running scripts/audit.sh or checking for any TODOs in code.

  - For UI features, if supported, use a browser automation to test (Antigravity’s browser agent could do this, or headless browser via MCP in Claude/Gemini).

  - Collect results: which tests passed/failed, any errors encountered.

  - If everything passes, mark steps as complete. If something fails, decide whether to fix it (which might loop back to Execute for that step or create a new fix step). In an autonomous loop, an agent might iteratively go back to Execution until Verify passes (this is akin to an agentic loop that continues until done – we saw an example of using a hook to re-loop until tests pass in Cursor).

Artifacts:

  - Test results output (could be captured in a file, e.g., artifacts/test_results.txt).

  - Screenshots or other artifacts for visual verification (Antigravity mentioned agents produce Artifacts like screenshots for UI changes).

  - An updated plan or report note if new tasks were added due to verification (e.g., “Step 4 failed, added Step 5 to fix bug”).

Example: After executing the backend and frontend, the verify agent runs all tests and finds a failing case for empty CSV files. It might then either fix it (if within capability) or at least log it. Suppose it fixes by adjusting code and retesting – this would be an Execute action within the Verify phase essentially. We expect our agents to handle minor fixes automatically. For bigger issues, they might signal for human input.

### Phase 5: Report

Goal: Present the outcome of the task clearly, either to the user or for record-keeping. Summarize what was done, confirm satisfaction of the request, and highlight any follow-ups.

Agent Actions:

  - Gather all relevant info: final status of each plan step, where the changes are (branch name or commit IDs), any remaining issues or suggestions.

  - Format a report. This could be a markdown summary to the user: “✅ Feature implemented: CSV upload now available. All tests passing. Committed to feature/csv-upload branch. Here’s a summary of changes: ...”.

  - Include pointers to artifacts: e.g., link to the pull request or attach the final diff (some agents can generate diff summaries automatically).

  - Ensure the report is concise and clear. Possibly separate sections: “What was done,” “Results,” “Next Steps (if any).”

  - If multiple agents were involved, the coordinator can compile their individual reports into one.

Required Output:

  - A REPORT.md (or comment in the chat) that contains the above summary.

  - In a non-interactive context, the report might be the final console output. In a collaborative setting, it could be stored in agent/artifacts/final_report.md.

The Report phase effectively closes the loop with the user, building trust by showing evidence of verification (Antigravity’s philosophy is to verify via Artifacts, not just logs– our report serves that role). It should be standardized enough that if different agents complete the same task, their reports are comparable (for consistency scoring).

Workflow Enforcement: We will enforce that agents do not skip these phases. For example:

  - Agents should not jump straight to writing code (Execute) without a plan. If they do, our evaluation framework (section 8) will flag it (protocol compliance score).

  - If an agent tries to produce a plan without clarifying obvious ambiguities, that’s an Intake failure.

  - Skipping Verify is not allowed; even if an agent is confident, it must at least run basic checks or explicitly state why verification is minimal (e.g., if it's a trivial change with no tests, then the verify step would note that).

All tools are instructed to abide by this via system prompts or memory files:

  - In Claude’s CLAUDE.md we might include: “All tasks must follow the 5-step workflow: intake, plan, execute, verify, report. Do not omit steps.”.

  - Similarly, Gemini’s GEMINI.md could have a short reminder of the phases.

  - Cursor’s rules can include a rule about workflow (ensuring even if user doesn’t explicitly prompt each, the agent self-adheres).

If an agent session is interrupted mid-workflow (e.g., user stops the agent after plan is done), another agent can resume at the next phase by consulting artifacts (like the plan file) rather than starting from scratch. This handoff is possible because the workflow outputs (like the plan) are tangible.

Error handling across phases:

  - If at Verify something is wrong, the agent can either go back to Execute or include a remediation in the Report (depending on autonomy level).

  - If at Report the agent realizes some acceptance criterion wasn’t met (“We did everything but forgot to update the user manual”), it could either add a step and loop again, or mention it. Ideally, such misses are caught in Plan or Verify, but the Report is last defense to note it.

By mandating this workflow, we align all agents with a common operational process, much like a software development methodology that all team members follow. This dramatically improves consistency: no matter if it’s Claude or Gemini doing the work, the user will observe the same pattern – first some clarifying questions, then a plan outline, then stepwise implementation, tests running, and a final summary. It also provides multiple places to catch and correct errors, improving reliability.

We will include a concise description of this protocol in the repository (likely in the agent/policies/WORKFLOW.md) and in any “system prompt” given to agents. For instance, the system prompt could say: “You are to function as a coding assistant following our 5-step workflow. Begin with Intake (clarify requirements), then Plan (outline tasks in YAML), then Execute (write code), Verify (test it), and Report (summarize). Adhere to this sequence strictly.” Ensuring the agent remembers this is crucial, hence the repetition in config files and skill instructions.

Now that we have the workflow defined, we will compare how each tool (Gemini, Claude, etc.) supports these instructions, highlighting differences and how the repository setup bridges those gaps.