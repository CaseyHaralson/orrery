## 10. Week 1 Implementation Checklist

To kickstart this project, we outline a checklist for the first week of implementation:

  - Repository Setup:

    1. Initialize the repository with the base structure:

        - Create agent/skills/ directory and subfolders for core skills (intake, plan, execute, verify, report, review, security).

        - Add placeholder SKILL.md files for each core skill with a basic outline (YAML frontmatter with name/description and a skeleton of sections).

        - Create agent/policies/WORKFLOW.md describing the 5-phase workflow in a few paragraphs (later to refine).

        - Create agent/schemas/plan-schema.json (draft as per section 4) and perhaps a stub for report-schema.json.

        - Set up .gemini, .claude, .cursor, .codex, .antigravity directories. Inside each, put a skills/ (can be empty for now or symlink to agent/skills if easy on your OS). Also add example config files:

          - .gemini/settings.json with "experimental.skills": true and any needed basic config.

          - .claude/settings.json with a basic allowed tools list (maybe allow Edit, Read, Write to project).

          - .cursor/rules/WORKFLOW.RULE.md (to persist the workflow reminder).

          - Others possibly empty or containing a README about how to configure if we don’t know yet.

    2. Version control: commit this initial structure. Also possibly set up a GitHub repository if not already, and configure branch protection if needed (to enforce reviews).

  - Tool Configuration & Testing:

    3. Install/set up latest versions of:

        - Claude Code CLI (or ensure access to Claude API).

        - Gemini CLI (register if needed, install via npm as docs suggest).

        - Cursor IDE (or CLI if exists; if only GUI, plan how to test maybe manually).

        - Confirm we have access to Google Antigravity (public preview download). If yes, install it and create a sample project to integrate with.

        - Ensure OpenAI API keys or access is available for Codex/GPT.

    4. For each tool, do a quick manual test in the project:

        - For Claude: Run claude in the repo directory. Ensure it picks up CLAUDE.md if present. We might not have content yet, but just verify it loads (the startup log often indicates loaded memory files).

        - For Gemini: Run gemini in the project. Check if it acknowledges skills (maybe run /skills list if such command exists, or see if it loaded our context).

        - For Cursor: Open the repo in Cursor editor, see if it recognizes the rules (type something to trigger it maybe). This might require more time, possibly skip deep test week1.

        - For OpenAI: Write a small Python script to simulate an agent using the plan skill. For example, prompt GPT-4: “You have skills X, you will plan etc.” to gauge response.

    5. Address any immediate issues (like if tools need a different file placement or naming to detect the skills).

  - Implement Sync Script:

    6. Write tools/sync_skills.py (or a Makefile) to copy agent/skills to each .*/skills. At first, it can be straightforward copy for all files. Run it and git-add the copied files. Commit “Sync initial skills to tool directories.”

    7. Test the symlink approach on one platform (maybe on a Unix-like system: ln -s agent/skills .gemini/skills). Decide if we include that or stick to copy. Document the choice.

  - Basic Skill Content Drafting:

    8. Flesh out at least the plan skill content with a first draft (since it’s central). For example, add the sections “When to use: at start of task”, “How to do: list steps...”, maybe a basic example. Use the info from this design.

    9. Similarly, draft execute and verify with a simple approach (like “Write code for the task. Then run tests.” etc.). These will be improved later but we want something to test end-to-end.

    10. intake skill: write a basic instruction like “Always restate the request and clarify anything unclear before proceeding.”

    11. report skill: instruct to summarize changes and outcomes.

    12. Keep these drafts short initially (we can refine with more detail after initial tests).

  - Simple End-to-End Test:

    13. Choose a trivial task (e.g., “Create a Python function add(a,b) with a test”). Try running through the workflow with one agent:

        - As user, input the request. Check that agent (say Claude via CLI) goes to Intake (maybe it asks a clarifying question or confirms).

        - If that works, have it proceed to Plan (maybe use --auto or just see if it creates a plan, if not, maybe we have to prompt “make a plan”).

        - This might require manually telling it phase by phase at first. This will reveal if our instructions are being heeded.

    14. If the agent doesn’t follow the workflow, tweak the approach: we might need to explicitly prompt it in this test (“Now do the Plan phase.”). That’s okay; note down how we can automate that with a coordinator later.

    15. Review outputs and adjust skill content as obvious (like if it made a poor plan, maybe our plan instructions need clarity).

  - Plan Schema Finalize:

    16. Finalize plan-schema.json based on what we want after seeing an example. Add a validation step in the sync or a separate script to validate a YAML plan file against it (maybe using a Python JSON schema library).

    17. Possibly create a agent/examples/sample_plan.yaml as a reference for developers, taken from our test.

  - CI Setup:

    18. Set up a minimal continuous integration workflow (GitHub Actions or similar) that:

        - Installs any dependencies (like Python for our scripts, maybe the skills-ref validator via pip if needed).

        - Runs tools/sync_skills.py and then checks for changes (to ensure sync was up-to-date).
        
        - Runs skills-ref validate agent/skills/* to validate skill frontmatters.

        - (Later, we can add actual agent runs or test tasks, but in week1 just ensure repository integrity).

    19. Also, perhaps run a markdown linter or YAML linter on our files to keep format tidy.

  - Governance Bootstrap:

    20. Create GitHub Issue templates for “New Skill Proposal” and “Skill/Workflow Change” to guide contributors to provide needed info.

    21. Document contribution guidelines in CONTRIBUTING.md: explain to propose changes via PR, require review, mention running sync and tests locally before PR.

    22. Set up codeowners or at least assign roles: e.g., certain senior devs as reviewers for skill changes, maybe security team as reviewer for high-risk skill changes.

  - Knowledge Share:

    23. Present this architecture to the team (maybe a meeting or an internal doc). Walk through how to add a skill, how an agent uses them, and where to find results.

    24. Ensure each team member has the needed access (API keys, installed tools) and environment to run the multi-agent system, so they can start experimenting in week 2.

By end of week 1, we should have:

  - The skeleton of the system in place, under version control.

  - At least one simple example of the whole workflow executed on one agent, proving viability.

  - Basic CI to guard structure.

  - Team alignment on how to proceed.

This primes us to then iterate: Week 2 could deepen each skill’s content and start complex testing; Week 3 could implement the coordinator automation for agent handoff; and so on.

Overall, following this checklist will create the foundation of the cross-agent skills and planning system, allowing us to progressively refine and expand it with confidence.