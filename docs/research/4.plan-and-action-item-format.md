## 4. Plan & Action Item Format (Agent-to-Agent Handoff)

To coordinate multiple agents and allow one to pick up where another left off, we define a Plan & Action Item format that is both machine-readable and human-readable. This format acts as an interchange document describing what needs to be done, by whom, and with what dependencies. We choose YAML (or JSON) for the structured format, because it’s easily parsed by programs and reasonably easy for humans (and AI models) to read and edit. YAML also allows adding comments or descriptions, which can be useful for humans supervising the plan.

### Plan Schema

A Plan consists of a list of action items (steps), each with attributes capturing the requirements from the prompt:

  - id: A unique identifier for the action (e.g., a number or string). Useful for referencing dependencies.

  - description: Natural language summary of the task to perform.

  - owner: The agent or role responsible for this step. This could be a specific tool/agent name (like "Claude" or "Gemini") or a role like "UI-Agent" or "DB-Agent" if we categorize by expertise. It can also be "Coordinator" for a step that involves orchestration.

  - status: The current status of the step – e.g., "pending", "in_progress", "complete", "blocked". This allows tracking partial completion.

  - deps: Dependencies – a list of ids that this step depends on. A step shouldn’t start until its dependencies are complete. This encodes ordering and parallelization.

  - parallel: (Optional boolean) If true, indicates this step can be done in parallel with others (assuming deps are satisfied). Otherwise, default sequential.

  - criteria: Acceptance criteria – what are the conditions of success for this step. This ensures whoever executes it knows when they’re “done.” (E.g., “unit tests for module X are all passing” or “UI passes manual smoke test”).

  - commands: (Optional) Specific commands or actions to execute as part of this step. For example, a step might include "commands: ['Run: npm test', 'Read: logs/output.log']" if it entails running tests. These can guide an executing agent or even be used by a runner to automate some checks.

  - files: (Optional) File targets – list of filenames that this step will create or modify. Useful for verifying changes or avoiding conflicts. For instance, step “Implement upload API” might list ['backend/api/upload.py', 'backend/api/__tests__/upload_test.py'].

  - risk_notes: (Optional) Any warnings or things to watch out for (“This step might affect authentication flow, be careful with user permissions”).

We formalize this in a JSON Schema (in agent/schemas/plan-schema.json). Here’s a simplified version of the schema definition:

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "AgentPlan",
  "type": "object",
  "properties": {
    "steps": {
      "type": "array",
      "items": { "$ref": "#/definitions/Step" }
    }
  },
  "definitions": {
    "Step": {
      "type": "object",
      "required": ["id", "description", "owner"],
      "properties": {
        "id": { "type": "string" },
        "description": { "type": "string" },
        "owner": { "type": "string" },
        "status": { "type": "string", "enum": ["pending","in_progress","complete","blocked"] },
        "deps": {
          "type": "array",
          "items": { "type": "string" }
        },
        "parallel": { "type": "boolean" },
        "criteria": { "type": "string" },
        "commands": {
          "type": "array",
          "items": { "type": "string" }
        },
        "files": {
          "type": "array",
          "items": { "type": "string" }
        },
        "risk_notes": { "type": "string" }
      }
    }
  }
}
```

This schema ensures a consistent structure. The agents (or a coordinator program) can validate any plan JSON/YAML against it.

### Example Plan File

Suppose the user asks for a new feature that spans front-end and back-end changes. An agent (say using the plan skill) produces a plan and saves it as current_plan.yaml. Here’s an example:

```yaml
# Plan generated on 2026-01-10 by PlannerAgent (Claude)
steps:
- id: "1"
  description: "Design the database schema for storing uploaded CSV data"
  owner: "DB-Agent"
  deps: []
  status: "pending"
  criteria: "Schema updated with a new table for CSV metadata"
  risk_notes: "Ensure not to break existing schema; consider migration"
- id: "2"
  description: "Implement backend endpoint to upload CSV and compute summary stats"
  owner: "Backend-Agent"
  deps: ["1"]
  status: "pending"
  criteria: "POST /upload returns summary (count, mean, median) for uploaded CSV"
  commands:
    - "Run: pytest tests/test_upload.py"   # to validate later
  files:
    - "backend/routes/upload.py"
    - "backend/services/csv_stats.py"
- id: "3"
  description: "Implement front-end UI for CSV upload (file picker and results display)"
  owner: "Frontend-Agent"
  deps: ["2"]           # front-end after backend is ready (or could be parallel with backend if stubbed)
  status: "pending"
  criteria: "User can select a file and see summary stats after upload"
  files:
    - "frontend/components/UploadWidget.vue"
- id: "4"
  description: "End-to-end integration test of the new upload feature"
  owner: "QA-Agent"
  deps: ["2", "3"]
  status: "pending"
  criteria: "Uploading a sample CSV through the UI yields the correct stats in response"
  commands:
    - "Run: playwright test e2e/upload.spec.ts"
  risk_notes: "Test on large CSV to ensure performance is acceptable"
```

This plan is both readable and parseable. A human can see what’s to be done, and an automated system or agent can load this structure. The plan indicates, for example, step 1 (DB schema) has no prerequisites and can start immediately, steps 2 and 3 depend on prior steps, etc. It also clearly assigns different “agents” (by role) to each step – which in a multi-agent setup could correspond to spinning up specialized agents or simply guiding a single agent with an indication of persona.

### Agent-to-Agent Handoff and Collaboration

With this plan, we enable several collaboration patterns:

  - Sequential handoff: One agent (Planner) created the plan. Now, another agent (Coordinator or the specific agent in owner) will execute each step. For example, when step 1 is ready, we could launch a DB-Agent (which might just be the same AI model prompted to focus on database tasks with relevant skill context) to carry out step 1. After step 1 completes and the plan is updated (mark status complete), the coordinator moves to step 2, launching Backend-Agent, and so on.

  - Parallel work: If two steps have no dependency relation (or are both ready after deps are done), the coordinator can assign them to different agents to execute in parallel. For instance, if frontend (3) didn’t depend on backend (2), we could do them concurrently. The plan’s parallel flags (or implicitly lack of deps) signal this possibility.

  - Ownership & expertise: The owner field lets us route tasks to the best agent. For example, if owner: "Frontend-Agent" and we have a Claude-based agent fine-tuned for front-end, the coordinator can direct that task there. In a simpler scenario, owner might just be an annotation and the same agent does all steps but behaves slightly differently knowing the context (e.g., uses different tools for front vs backend).

  - Status tracking: As steps progress, agents or the coordinator update the status field. This could be done by editing the YAML file or keeping an internal state. For reliability, the coordinator agent could maintain an authoritative version of the plan file.

The plan file is machine-readable – e.g., a Python script or the coordinator agent can parse the YAML to ensure all dependencies are met before launching a step, and to locate which files or commands are expected. It’s also human-readable, so developers supervising the process can understand what the AI intends to do (and potentially intervene by editing the plan or adding comments if something looks off).

### Execution Report Format

In addition to plans, we define an Execution Report format, which is generated after (or during) the Execute and Verify phases. The purpose of the report is to summarize what actions were taken and the results, in a structured way. This helps both in verifying outcomes and in communicating between agents or to the user.

A report might include:

  - Which steps have been completed, and any evidence (like test results, or diff of code).

  - If a step failed or was blocked, include error messages or reasons.

  - Summary of any artifacts produced (screenshots, logs).

For example, after step 2 (implement backend) an agent might produce a partial report report_step2.json:

```json
{
  "step_id": "2",
  "outcome": "success",
  "details": "Backend endpoint /upload implemented and returns correct summary stats.",
  "artifacts": [
    {"file": "backend/routes/upload.py", "status": "created"},
    {"file": "backend/services/csv_stats.py", "status": "created"},
    {"test": "tests/test_upload.py", "result": "PASSED in 0.42s"}
  ],
  "timestamp": "2026-01-10T17:30:00Z"
}
```
We could have a report per step or a combined report for the whole task. In CI or evaluation, these reports can be checked against expectations (e.g., did all tests pass? was the code diff within acceptable size? etc.).

The plan and report together enable a closed-loop collaboration: The plan tells agents what to do, and the report (plus environment feedback like test results) tells whether it was done correctly, which in turn could update the plan (if something needs redoing or additional steps added – e.g., a bug fix step could be appended if an error is found, akin to an iterative loop).

### Multi-Agent Pattern: Coordinator + Workers

To orchestrate this, we envision a Coordinator Agent that serves as the overseer. The Coordinator’s responsibilities:

  1. Read/maintain the plan structure.

  2. Decide which step(s) to execute next (based on dependencies and statuses).

  3. Dispatch each step to an appropriate Worker agent (which could simply be invoking the right skill set in the same LLM, or spinning off a separate session possibly with a different model).

  4. Monitor progress and update the plan status.

  5. Handle any feedback or new information (for example, if a Worker reports that a step failed, the Coordinator might insert a new remedial step or mark the plan as needing review).

Worker Agents are essentially instances that execute individual steps. They use the Skills library to actually carry out the task:

  - If the step is coding, the worker uses the execute skill (and likely verify as part of it).

  - If the step is verification or testing (like step 4), the worker might primarily use a verify skill.

  - If a step is a review, a worker with the review skill could perform it.

This pattern is very much like a manager delegating to employees. It aligns with research patterns of using a Planner (or Orchestrator) and Executors in LLM systems. In fact, instructing an LLM to output a structured plan and then following it is a known approach for complex tasks. Our system formalizes that with a persistent plan file and multiple agents.

Example Collaboration: Let’s walk through a short scenario:

  - The Intake phase agent (could be the same as coordinator or a dedicated one) takes the user prompt and possibly asks clarifying questions (guided by intake skill). Suppose the user says “Add CSV upload feature.” The intake agent confirms requirements and yields a refined task description.

  - The Plan phase agent (Planner) uses the plan skill to create current_plan.yaml (like the example above). This plan is saved and maybe also summarized to the user (“I have outlined 4 steps: DB schema, Backend API, Frontend UI, Integration Test.”).

  - The Coordinator reads the plan. Step 1 has no deps, so it selects an agent (maybe it knows a template prompt for a DB-specialized agent, or it uses the same model but primes it with the execute skill and context focusing on DB).

  - The Worker for step 1 executes: using execute skill to write migration or schema changes. It marks step 1 done, possibly attaches a short report (e.g., “added table X, updated ERD diagram”).

  - Coordinator updates current_plan.yaml step 1 to status "complete". Now sees step 2 is unblocked. It dispatches a Worker for step 2 (backend).

  - Step 2 worker writes code, runs tests. Suppose tests pass; it marks success. If tests failed, step 2 worker might update status to "blocked" and even append a sub-step in the plan for fixing the test (depending on autonomy – or it could try to fix within the same step).

  - Coordinator sees step 2 done, triggers step 3 (frontend agent). Meanwhile, conceivably, it could have triggered step 3 earlier in parallel if feasible.

  - Eventually all steps complete, then maybe the coordinator triggers a final Report phase agent to compile a comprehensive report for the user or for records, using the report skill (which might gather all step reports and produce a markdown summary or commit log).

Throughout, each agent is using the same protocol and skills: they produce plans, write code, verify, etc., in the prescribed formats. This means any agent could hand over to another at a clean break. For example, even if one tool (say Codex) started the plan and for some reason had to stop, another (say Claude Code) could read current_plan.yaml and continue execution, because it understands the same schema and has the same skills available to interpret “Implement backend endpoint” in the same way.

Human-Readable Aspect: The plan is kept human-readable intentionally. If a developer wants to modify it (“Actually, add a step 5 to update documentation”), they can edit the YAML and commit it or feed it back to the coordinator agent. The agents will treat it as the new source of truth. Also, if something goes wrong, a human can inspect the plan and partial outputs to debug the agent’s reasoning. This transparency is a big advantage over a pure end-to-end black-box approach.

We will provide templates and examples of plan and report files in the repository (perhaps in agent/examples/). During development, we can test the plan generation and consumption logic with these examples to ensure robustness.

To summarize, the Plan/Action format is the lingua franca for our multi-agent system, enabling planned coordination, parallel execution, and clear tracking of work. It is a crucial component for achieving consistent outcomes no matter which agent is actually doing the work, and for supporting agent hand-offs gracefully.