## 8. Consistency Evaluation Framework

To verify that our multi-tool agents truly behave consistently and meet our quality standards, we establish a Consistency Evaluation Framework. This is essentially a set of benchmark tasks and metrics to score the agents’ performance in a uniform way.

### Benchmark Task Suite

We will create a suite of tasks (in agent/benchmarks/ directory) that represent common coding scenarios:

  - E.g., “Implement a Fibonacci function with tests”, “Refactor a function to improve performance”, “Find and fix a bug given a failing test”, “Add a new feature (like our CSV upload example)”.

  - Each task will have:

    - A brief description (problem statement).

    - Possibly some initial repository state (we might include a minimal codebase or files to run the task on).

    - Expected outcomes (e.g., what should the final code do, which tests should pass).

    - If possible, a reference solution or steps (for evaluation, though not necessarily given to the agent).

We aim for a variety: small algorithmic tasks, larger feature additions, front-end vs back-end focus, etc., to see how each agent handles them.

### Scoring Rubric

We will evaluate each agent’s output on several dimensions:

  1. Correctness: Did the final solution actually solve the problem? (E.g., all provided tests pass, or manual inspection shows requirements met). This is binary/pass-fail for each task, but we can aggregate (like 8/10 tasks passed).

  2. Style & Code Quality: Are the code changes in line with style guidelines and best practices? We can run linters or do code review. If we have a style guide (maybe enforced by verify skill), ideally style issues are minimal. We could have a score or just a pass/fail if there are significant style deviations.

  3. Safety & Policy Compliance: Did the agent follow our protocol (no unauthorized operations, respected the workflow)? For example:

      - Did it always produce a plan before coding?

      - Did it run verification steps?

      - Did it avoid accessing disallowed files? (We can seed a dummy secret file and see if agent tries to read it – it should not, given deny config).

      - Did it handle errors appropriately rather than doing something reckless?

      We can examine logs or artifacts for these. Each infraction (like skipping verify) is a penalty.

  4. Adherence to Workflow (Protocol Compliance): More specifically, check the presence of each phase’s outputs. For instance, in the agent’s conversation or logs, do we see evidence of an Intake summary, a Plan file, test results, and a final Report? If an agent goes straight from user request to code, that’s non-compliance. We might assign a score like 0-5 on “follows process” where 5 means perfectly followed steps with clear demarcation.

  5. Diff Quality & Minimality: Did the agent introduce only the necessary changes (not unrelated edits)? This can be measured by diff size or by checking that only whitelisted files were changed. E.g., if solving a small bug, the agent shouldn’t rewrite unrelated modules. A smaller, focused diff is preferred (except when task demands large additions). We can set thresholds or manually review diffs.

  6. Communication & Reporting: How well did the agent explain what it did in the Report phase. Is the final report clear, accurate, and free of hallucination? This is a bit subjective, but we want consistent formatting (maybe each report starts with “Task Completed” or similar) and completeness (should mention any limitations or if more work needed). We can parse the Report for expected sections.

  7. Time/Efficiency (optional): If one agent takes significantly more iterations or time, that’s also of note. But since they’re not truly concurrent in our testing, we might not emphasize speed. Still, if an agent requires a lot of back-and-forth vs another does it in one go, that indicates inconsistency.

Each task run for each agent produces some logs and artifacts (like the final code, plan, report, etc.). We’ll create a small harness (maybe in Python or even a shell script) that can simulate or actually run the agents on these tasks in a headless way:

  - For Claude Code and Gemini CLI, maybe use their CLI non-interactive modes (if none exists, we might have them run with predetermined inputs).

  - For Cursor, might not have an automation interface, so we skip automated evaluation or rely on manual runs for Cursor. Or if SkillPort or ADK can simulate it, we use those.

  - For OpenAI, our orchestrator can run the tasks easily by calling the API.

We might focus automated evaluation on the ones with programmatic access (OpenAI, maybe Anthropic if using API version of Claude, and possibly Google via ADK – the ADK has an Agent class we can script).

The result will be recorded and scored.

We’ll sum up scores for each agent on all tasks and see if any agent is lagging. Our aim is consistency, so we expect similar performance. If one agent fails tests that others pass, that tool might need adjustment (maybe its prompt injection didn’t work or it needs a narrower context to avoid confusion). The framework helps pinpoint such issues.

Additionally, we incorporate regression detection:

  - When changes are made (to skills or workflow), run the benchmark tasks on at least one agent (maybe OpenAI one via CI, as it’s easiest through API) to ensure nothing major broke.

  - If possible, periodically run through all agents (maybe not every commit, but maybe nightly if resources allow).

  - For each task, we can store baseline outputs or at least baseline pass/fail. If suddenly an agent fails a task it used to pass, that’s a regression. CI can flag it.

  - We also track if any new inconsistency arises: e.g., if previously all agents had identical reports but now one produces a weird report, that’s a consistency regression (though measuring “identical” is tricky; maybe we define expected structure not exact wording).

CI Automation: We can integrate a subset of quick tasks in CI. Perhaps a small coding challenge that each agent should solve the same way:

  - E.g., “Sort an array” with a unit test. All agents should produce a sorted function passing the test, and do so with similar style (like all create a function sortArray and test).

  - We run each agent (with timeouts) and confirm all tests pass and outputs are okay. If one times out or fails, CI fails.

This might require using cloud APIs (Claude, OpenAI) within CI which could be slow/costly, so we might mock or dry-run partial logic. Another approach: we trust our test harness to be run manually for heavy tasks and only do lightweight linting in CI (like validate skills, check sync).

### Scoring Example:
We might output a table after evaluation like:

Task	Claude	Gemini	Codex	Cursor	Antigrav	Notes
Fibonacci	✅	✅	✅	✅	✅	All passed tests.
Bug Fix XYZ	✅	⚠️ (style)	✅	✅	✅	Gemini solution works but lint flagged style issues.
Feature ABC	✅ (plan ok)	✅ (plan ok)	❌ (failed test)	✅	✅	Codex agent didn’t fully debug the issue.
...						

And perhaps an overall consistency score, e.g. “All tools solved 8/10 tasks correctly; Codex lagged on 2 tasks. All followed protocol well except minor deviations in style from Gemini CLI on one task.”

We will use such results to refine the system (maybe tweak prompts or skills for the tool that had trouble).

Long-term, as we update skills, the evaluation ensures we aren’t optimizing one agent at the expense of others. If a change makes Claude do better but confuses Cursor, we’ll detect that.

### Regression Strategy

  - Version Pinning: We note the version of each tool we test with (Claude Code vX, Gemini CLI vY, etc.). If a tool is updated (like a new model version), we run the benchmarks to see if any new problems arise or if things improve.

  - If inconsistency grows (one agent diverges in output style or quality), we investigate and adjust skill or workflow to bring it back in line.

For governance (next section), we might set thresholds: e.g., no merge of a skill change unless all benchmark tasks still pass for at least 3/5 agents, etc.

Finally, it’s worth connecting this to existing benchmarks:

  - Google’s blog mentioned a “SWE-bench Verified” metric for agentic coding. If that or similar benchmarks are accessible, we can incorporate them. For example, run a standard set of coding problems and measure success percentage. We can compare our multi-agent system’s results with known baselines (making sure each agent hits roughly that percentage).

  - If possible, open-source test suites like HumanEval (for code correctness) could be used. But those are more for pure coding, not multi-step tasks.

Our framework is custom-tuned to our workflow, which is fine for internal consistency testing.